{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain\n",
        "# !pip install openai"
      ],
      "metadata": {
        "id": "PCp5Yb5Sv8sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "z5SUU_Y-l-0l"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import openai\n",
        "import os\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "# from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect the API keys\n",
        "path = \"/content/ChatGPTapiKey.txt\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = open(path,\"r\").read().split(\"\\n\")[0]"
      ],
      "metadata": {
        "id": "J-_AH8M4nO9A"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm1 = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.8) # model = gpt-3.5-turbo\n",
        "# llm2 = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.8) # model = gpt-3.5-turbo"
      ],
      "metadata": {
        "id": "KCf8TBd8m_m3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv1(llm):\n",
        "  breakPoint=\"exit\"\n",
        "  while(1):\n",
        "    prompt = input(\"Query: \")\n",
        "    if breakPoint == prompt.lower():\n",
        "      print(\"Exit!\")\n",
        "      break\n",
        "    response = llm(prompt)\n",
        "    print(\"Response: \", response)"
      ],
      "metadata": {
        "id": "EqHm5QSqm_aH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv1(llm1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBGxZ92kwSnB",
        "outputId": "4db87b9a-9e82-4905-9d84-9498be4bbebf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: hi\n",
            "Response:  Hello! How can I assist you today?\n",
            "Query: how are you?\n",
            "Response:  I'm an AI, so I don't have feelings, but I'm here to help you with anything you need. How can I assist you today?\n",
            "Query: are you fine ?\n",
            "Response:  As an AI language model, I do not have emotions or physical sensations, so I do not experience being fine or not. However, thank you for asking. How may I assist you today?\n",
            "Query: exit\n",
            "Exit!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# conv1(llm2)"
      ],
      "metadata": {
        "id": "r1FWUJyum_Xy"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                                                          -:END:-"
      ],
      "metadata": {
        "id": "maw2-U1Fxfon"
      }
    }
  ]
}